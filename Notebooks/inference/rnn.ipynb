{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T17:54:45.057544Z",
     "start_time": "2025-11-22T17:54:44.976141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "3c5c62de8dea6a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-22T17:54:45.185770Z",
     "start_time": "2025-11-22T17:54:45.098896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar el dataset\n",
    "from utils.lsa64.dataset import LSA64Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "dataset = LSA64Dataset(\"../../data/LSA64/landmarks\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])"
   ],
   "id": "46a9b0d2475806d5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T17:54:45.295053Z",
     "start_time": "2025-11-22T17:54:45.210985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Cargarlo al GPU, random_split ya lo shufflea, padearlo con 0s asi tienen el mismo tamaño(al entrenar se ignoran esos 0s)\n",
    "def collate_pad(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    lengths = torch.tensor([x.size(0) for x in xs])\n",
    "    x_padded = pad_sequence(xs, batch_first=True, padding_value=0.0)\n",
    "    y_tensor = torch.tensor(ys)\n",
    "    return x_padded, lengths, y_tensor\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=6, collate_fn=collate_pad) # Shufflearlo por cada epoch\n",
    "test_loader = DataLoader(test_ds, batch_size=32, num_workers=6, collate_fn=collate_pad)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ],
   "id": "7ed655e3c2becddb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T17:54:45.413974Z",
     "start_time": "2025-11-22T17:54:45.339964Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset[0][0].shape)",
   "id": "8570e19d7628b40f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 177])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T19:44:18.668221Z",
     "start_time": "2025-11-22T19:44:18.643527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=177,\n",
    "            hidden_size=177,\n",
    "            num_layers=5,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(177, 64)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # lengths must be on CPU for pack_padded_sequence\n",
    "        lengths = lengths.cpu()\n",
    "\n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        packed_out, h = self.rnn(packed)\n",
    "        # h: (num_layers, batch, hidden_size)\n",
    "        last = h[-1]                      # final layer’s hidden state\n",
    "\n",
    "        logits = self.linear(last)\n",
    "        return logits"
   ],
   "id": "e283b8f99b139266",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T19:46:23.325149Z",
     "start_time": "2025-11-22T19:44:19.385632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCH = 100\n",
    "\n",
    "model = SimpleRNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # ======================\n",
    "    # TRAIN\n",
    "    # ======================\n",
    "    model.train()\n",
    "    for x, lengths, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x, lengths)\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ======================\n",
    "    # TRAIN ERROR (small subset)\n",
    "    # ======================\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_batches = 5   # <--- amount of batches to sample\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, lengths, y) in enumerate(train_loader):\n",
    "            if i >= train_batches:\n",
    "                break\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x, lengths)\n",
    "            loss = criterion(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            _, predicted = y_pred.max(1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "            train_total += y.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "\n",
    "    # ======================\n",
    "    # TEST ERROR\n",
    "    # ======================\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, lengths, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x, lengths)\n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = y_pred.max(1)\n",
    "            test_correct += (predicted == y).sum().item()\n",
    "            test_total += y.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100 * test_correct / test_total\n",
    "\n",
    "    # ======================\n",
    "    # PRINT SUMMARY\n",
    "    # ======================\n",
    "    print(f\"Epoch {epoch+1}/{EPOCH} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")"
   ],
   "id": "cfed47b15c38ecfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Acc: 3.75% | Test Acc: 2.79%\n",
      "Epoch 2/100 | Train Acc: 6.25% | Test Acc: 6.70%\n",
      "Epoch 3/100 | Train Acc: 12.50% | Test Acc: 8.10%\n",
      "Epoch 4/100 | Train Acc: 13.75% | Test Acc: 12.01%\n",
      "Epoch 5/100 | Train Acc: 15.62% | Test Acc: 13.13%\n",
      "Epoch 6/100 | Train Acc: 21.88% | Test Acc: 15.92%\n",
      "Epoch 7/100 | Train Acc: 23.12% | Test Acc: 19.83%\n",
      "Epoch 8/100 | Train Acc: 21.25% | Test Acc: 18.72%\n",
      "Epoch 9/100 | Train Acc: 26.25% | Test Acc: 21.79%\n",
      "Epoch 10/100 | Train Acc: 29.38% | Test Acc: 27.09%\n",
      "Epoch 11/100 | Train Acc: 35.62% | Test Acc: 31.84%\n",
      "Epoch 12/100 | Train Acc: 35.62% | Test Acc: 32.40%\n",
      "Epoch 13/100 | Train Acc: 34.38% | Test Acc: 28.77%\n",
      "Epoch 14/100 | Train Acc: 40.62% | Test Acc: 31.01%\n",
      "Epoch 15/100 | Train Acc: 38.75% | Test Acc: 34.92%\n",
      "Epoch 16/100 | Train Acc: 38.12% | Test Acc: 39.11%\n",
      "Epoch 17/100 | Train Acc: 45.62% | Test Acc: 38.55%\n",
      "Epoch 18/100 | Train Acc: 46.25% | Test Acc: 37.71%\n",
      "Epoch 19/100 | Train Acc: 40.00% | Test Acc: 41.62%\n",
      "Epoch 20/100 | Train Acc: 40.62% | Test Acc: 41.90%\n",
      "Epoch 21/100 | Train Acc: 51.25% | Test Acc: 47.21%\n",
      "Epoch 22/100 | Train Acc: 59.38% | Test Acc: 49.72%\n",
      "Epoch 23/100 | Train Acc: 49.38% | Test Acc: 47.77%\n",
      "Epoch 24/100 | Train Acc: 53.12% | Test Acc: 48.04%\n",
      "Epoch 25/100 | Train Acc: 50.00% | Test Acc: 48.32%\n",
      "Epoch 26/100 | Train Acc: 61.25% | Test Acc: 54.47%\n",
      "Epoch 27/100 | Train Acc: 59.38% | Test Acc: 53.63%\n",
      "Epoch 28/100 | Train Acc: 73.12% | Test Acc: 56.98%\n",
      "Epoch 29/100 | Train Acc: 63.12% | Test Acc: 55.59%\n",
      "Epoch 30/100 | Train Acc: 65.62% | Test Acc: 58.10%\n",
      "Epoch 31/100 | Train Acc: 71.25% | Test Acc: 61.73%\n",
      "Epoch 32/100 | Train Acc: 70.62% | Test Acc: 62.85%\n",
      "Epoch 33/100 | Train Acc: 68.12% | Test Acc: 62.57%\n",
      "Epoch 34/100 | Train Acc: 68.75% | Test Acc: 63.69%\n",
      "Epoch 35/100 | Train Acc: 74.38% | Test Acc: 66.20%\n",
      "Epoch 36/100 | Train Acc: 81.25% | Test Acc: 66.20%\n",
      "Epoch 37/100 | Train Acc: 71.88% | Test Acc: 67.60%\n",
      "Epoch 38/100 | Train Acc: 76.88% | Test Acc: 68.16%\n",
      "Epoch 39/100 | Train Acc: 72.50% | Test Acc: 65.36%\n",
      "Epoch 40/100 | Train Acc: 76.88% | Test Acc: 72.07%\n",
      "Epoch 41/100 | Train Acc: 81.25% | Test Acc: 65.64%\n",
      "Epoch 42/100 | Train Acc: 75.00% | Test Acc: 68.99%\n",
      "Epoch 43/100 | Train Acc: 85.62% | Test Acc: 72.35%\n",
      "Epoch 44/100 | Train Acc: 81.25% | Test Acc: 71.79%\n",
      "Epoch 45/100 | Train Acc: 82.50% | Test Acc: 70.11%\n",
      "Epoch 46/100 | Train Acc: 86.88% | Test Acc: 72.07%\n",
      "Epoch 47/100 | Train Acc: 87.50% | Test Acc: 75.14%\n",
      "Epoch 48/100 | Train Acc: 83.12% | Test Acc: 75.42%\n",
      "Epoch 49/100 | Train Acc: 81.88% | Test Acc: 76.26%\n",
      "Epoch 50/100 | Train Acc: 85.62% | Test Acc: 75.14%\n",
      "Epoch 51/100 | Train Acc: 84.38% | Test Acc: 74.30%\n",
      "Epoch 52/100 | Train Acc: 82.50% | Test Acc: 76.82%\n",
      "Epoch 53/100 | Train Acc: 84.38% | Test Acc: 72.07%\n",
      "Epoch 54/100 | Train Acc: 83.12% | Test Acc: 75.98%\n",
      "Epoch 55/100 | Train Acc: 93.12% | Test Acc: 75.42%\n",
      "Epoch 56/100 | Train Acc: 85.62% | Test Acc: 78.21%\n",
      "Epoch 57/100 | Train Acc: 85.62% | Test Acc: 76.54%\n",
      "Epoch 58/100 | Train Acc: 86.25% | Test Acc: 75.70%\n",
      "Epoch 59/100 | Train Acc: 92.50% | Test Acc: 78.21%\n",
      "Epoch 60/100 | Train Acc: 93.12% | Test Acc: 79.05%\n",
      "Epoch 61/100 | Train Acc: 91.88% | Test Acc: 79.89%\n",
      "Epoch 62/100 | Train Acc: 88.12% | Test Acc: 79.89%\n",
      "Epoch 63/100 | Train Acc: 92.50% | Test Acc: 77.65%\n",
      "Epoch 64/100 | Train Acc: 88.75% | Test Acc: 79.89%\n",
      "Epoch 65/100 | Train Acc: 91.88% | Test Acc: 81.01%\n",
      "Epoch 66/100 | Train Acc: 89.38% | Test Acc: 83.80%\n",
      "Epoch 67/100 | Train Acc: 90.62% | Test Acc: 77.93%\n",
      "Epoch 68/100 | Train Acc: 91.25% | Test Acc: 78.49%\n",
      "Epoch 69/100 | Train Acc: 91.25% | Test Acc: 79.89%\n",
      "Epoch 70/100 | Train Acc: 95.00% | Test Acc: 84.36%\n",
      "Epoch 71/100 | Train Acc: 93.12% | Test Acc: 80.73%\n",
      "Epoch 72/100 | Train Acc: 91.25% | Test Acc: 80.73%\n",
      "Epoch 73/100 | Train Acc: 95.62% | Test Acc: 83.52%\n",
      "Epoch 74/100 | Train Acc: 88.75% | Test Acc: 81.56%\n",
      "Epoch 75/100 | Train Acc: 94.38% | Test Acc: 84.36%\n",
      "Epoch 76/100 | Train Acc: 95.62% | Test Acc: 83.24%\n",
      "Epoch 77/100 | Train Acc: 90.00% | Test Acc: 81.56%\n",
      "Epoch 78/100 | Train Acc: 92.50% | Test Acc: 81.56%\n",
      "Epoch 79/100 | Train Acc: 91.25% | Test Acc: 83.24%\n",
      "Epoch 80/100 | Train Acc: 88.75% | Test Acc: 82.40%\n",
      "Epoch 81/100 | Train Acc: 91.25% | Test Acc: 81.84%\n",
      "Epoch 82/100 | Train Acc: 95.62% | Test Acc: 86.31%\n",
      "Epoch 83/100 | Train Acc: 96.25% | Test Acc: 86.31%\n",
      "Epoch 84/100 | Train Acc: 95.62% | Test Acc: 84.08%\n",
      "Epoch 85/100 | Train Acc: 93.12% | Test Acc: 83.24%\n",
      "Epoch 86/100 | Train Acc: 95.00% | Test Acc: 87.99%\n",
      "Epoch 87/100 | Train Acc: 96.25% | Test Acc: 84.92%\n",
      "Epoch 88/100 | Train Acc: 96.25% | Test Acc: 88.27%\n",
      "Epoch 89/100 | Train Acc: 95.00% | Test Acc: 87.15%\n",
      "Epoch 90/100 | Train Acc: 95.00% | Test Acc: 87.43%\n",
      "Epoch 91/100 | Train Acc: 90.00% | Test Acc: 82.96%\n",
      "Epoch 92/100 | Train Acc: 96.25% | Test Acc: 87.43%\n",
      "Epoch 93/100 | Train Acc: 93.12% | Test Acc: 86.59%\n",
      "Epoch 94/100 | Train Acc: 94.38% | Test Acc: 82.40%\n",
      "Epoch 95/100 | Train Acc: 95.00% | Test Acc: 87.15%\n",
      "Epoch 96/100 | Train Acc: 96.88% | Test Acc: 87.99%\n",
      "Epoch 97/100 | Train Acc: 99.38% | Test Acc: 89.94%\n",
      "Epoch 98/100 | Train Acc: 95.62% | Test Acc: 88.83%\n",
      "Epoch 99/100 | Train Acc: 95.62% | Test Acc: 86.87%\n",
      "Epoch 100/100 | Train Acc: 95.62% | Test Acc: 85.47%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Por ahora me gusta, pero quiero encontrar formas de mejorar la generalizacion porque cuando tenga datasets peores, ahi si voy a tener mucho mas overfitting.\n",
    "\n",
    "En un paper lei que tuvieron 94% de accuracy, voy a intentar llegar a eso.\n",
    "# Resultados\n",
    "- RNN simple, tanh, 5 capas ocultas y 1 de salida, adam, lr=1e-4, lambda=1e-3: 99.38%, 89.94%\n",
    "\n",
    "# Ideas para probar\n",
    "- **REVISAR QUE MODELOS Y ARQUITECTURAS QUE HACEN ESTO YA EXISTEN**\n",
    "- La idea de PCA para dataset augmentation\n",
    "- Transfer learning usando otro lenguaje de señas similar al argentino pero mas completo\n",
    "-   Freezar las conexiones recurrentes? Freezar las primeras capas?\n",
    "- Un par de capas ocultas mas\n",
    "- Un par de capas ocultas no recurrentes con ReLU\n",
    "- Usar una GRU o echo state network para mayor memoria a largo plazo(me parece al pedo una LSTM porque si ya con este nivel de memoria se maneja bien, entonces no creo necesitar tanto control)\n",
    "- Encontrar una representacion menos ruidosa para los datos(el z lo infiere mal mediapipe en teoria, y la escala de las manos interpoladas tienen mucho ruido)\n",
    "- Quizas reducir los fps a 6 o menos\n",
    "- Aumentar el tamaño de los batches\n",
    "- Reducir el learning rate a $10^{-4}$ una vez que llegue ~80% porque oscila mucho\n",
    "  - Fijarme en un grafico el punto donde empieza a oscilar\n",
    "  - No graficar simplemente el train error, graficar la diferencia porcentual entre el train error anterior y este"
   ],
   "id": "f04998b1739980cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c2066ffff8691481"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
