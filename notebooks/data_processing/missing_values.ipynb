{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "ac920ad86ad019e5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exp 1: Reemplazo de datos missing\n",
    "Primero quiero testear quien agarra con mas accuracy, hands + pose o hollistic"
   ],
   "id": "bc949b62fc3253d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T20:47:39.747686Z",
     "start_time": "2025-11-19T20:47:38.616157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.mp_utils.render import render_frame, render_holistic_frame\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from utils.video import frame_reader\n",
    "import mediapipe as mp\n",
    "import cv2"
   ],
   "id": "5eef9b22ca7821ca",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T19:36:26.680392Z",
     "start_time": "2025-11-19T19:36:26.628853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cap_rh = cv2.VideoCapture(\"../../data/LSA64/video/001_001_001.mp4\")\n",
    "cap_bh = cv2.VideoCapture(\"../../data/LSA64/video/031_001_001.mp4\")"
   ],
   "id": "d8cc14b61d0dfe32",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T19:36:42.787111Z",
     "start_time": "2025-11-19T19:36:42.468260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prueba de render\n",
    "with mp.solutions.pose.Pose() as pose, mp.solutions.hands.Hands(max_num_hands=2) as hands, \\\n",
    "     ThreadPoolExecutor(max_workers=2) as ex:\n",
    "\n",
    "    for frame in frame_reader(cap_bh, fps=12):\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        fut_pose  = ex.submit(pose.process, rgb)\n",
    "        fut_hands = ex.submit(hands.process, rgb)\n",
    "\n",
    "        pose_results  = fut_pose.result()\n",
    "        hands_results = fut_hands.result()\n",
    "\n",
    "        render_frame(frame, pose_results, hands_results)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "57987ade24b0b5ec",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     11\u001B[39m         pose_results  = fut_pose.result()\n\u001B[32m     12\u001B[39m         hands_results = fut_hands.result()\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m         \u001B[43mrender_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpose_results\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhands_results\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m cv2.destroyAllWindows()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Proyectos\\SignTranslatorV2\\utils\\mp_utils\\render.py:51\u001B[39m, in \u001B[36mrender_frame\u001B[39m\u001B[34m(frame, pose_array, left_hand_array, right_hand_array, window_name)\u001B[39m\n\u001B[32m     49\u001B[39m \u001B[38;5;66;03m# Pose\u001B[39;00m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pose_array \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m     annotated = \u001B[43mdraw_landmarks_from_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m        \u001B[49m\u001B[43mannotated\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpose_array\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconnections\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPOSE_CONNECTIONS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolor\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m255\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[38;5;66;03m# Mano izquierda\u001B[39;00m\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m left_hand_array \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Proyectos\\SignTranslatorV2\\utils\\mp_utils\\render.py:25\u001B[39m, in \u001B[36mdraw_landmarks_from_array\u001B[39m\u001B[34m(image, landmarks_array, connections, color, radius, thickness)\u001B[39m\n\u001B[32m     22\u001B[39m h, w, _ = image.shape\n\u001B[32m     23\u001B[39m points = []\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlandmarks_array\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Soporta tanto np.array/lista como objetos MediaPipe Landmark\u001B[39;49;00m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mx\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43my\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_norm\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm\u001B[49m\u001B[43m.\u001B[49m\u001B[43my\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: 'type' object is not iterable"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T19:37:19.216504Z",
     "start_time": "2025-11-19T19:37:15.553057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prueba de render usando holistic, a ver si captura mejor la info de las manos\n",
    "with mp.solutions.holistic.Holistic() as holistic:\n",
    "    for frame in frame_reader(cap_bh, fps=12):\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        holistic_res = holistic.process(rgb)\n",
    "        render_holistic_frame(frame, holistic_res)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "7d3abff2a4d97fb7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Resultados\n",
    "Parece que holistic captura todos los landmarks con mayor confianza que hands + pose, pero tengo que probar un poco mas. Por ahora me voy a quedar con holistic porque es la solucion mas facil y siento que deberia tener mejor accuracy que hands + pose"
   ],
   "id": "486673035632761d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exp 2: Reemplazo de datos missing\n",
    "Lo que quiero hacer es probar que puedo hacer cuando no llego a capturar la pose o la mano. Mi idea es trabajar con un metodo de distancias para las manos y pose donde\n",
    "- Si estoy a menos de n frames de un frame capturado, entonces usar los landmarks de ese frame\n",
    "- Si no, entonces\n",
    "  - Descartarlo para la pose\n",
    "  - Para las manos, reemplazar los landmarks missing con\n",
    "    - Sus puntos anteriores\n",
    "    - El template de la mano relajada\n",
    "\n",
    "Y para que quiero implementar eso si los videos son cortos y el sujeto siempre esta en el frame? Porque en el mundo real se puede ir del frame y va a durar mas. Por eso N tiene que ser grande\n",
    "\n",
    "Solo quiero usar el template de la mano relajada si nunca vi la otra mano, porque si no el modelo va a ver un salto raro de pose a relajado. Aun asi tengo que implementar eso de \"a los n frames olvidate\" para el mundo real"
   ],
   "id": "1a636785183cc8bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T11:36:57.705926Z",
     "start_time": "2025-11-20T11:36:53.435331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "#rec.render()"
   ],
   "id": "12933f3156b59475",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T11:37:56.015435Z",
     "start_time": "2025-11-20T11:37:55.919701Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "990c52c162295a8c",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mrec\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Proyectos\\SignTranslatorV2\\utils\\LSA64_parser.py:39\u001B[39m, in \u001B[36mVideoLSA64.render\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     37\u001B[39m pose, left, right = \u001B[38;5;28mnext\u001B[39m(parsed_landmarks)\n\u001B[32m     38\u001B[39m landmarks = np.concatenate((pose, left, right), axis=\u001B[32m0\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m \u001B[43mdraw_landmarks_from_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrgb_frame\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlandmarks\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Proyectos\\SignTranslatorV2\\utils\\mp_utils\\render.py:32\u001B[39m, in \u001B[36mdraw_landmarks_from_array\u001B[39m\u001B[34m(image, landmarks_array, connections, color, radius, thickness)\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     30\u001B[39m     x_norm, y_norm = lm[\u001B[32m0\u001B[39m], lm[\u001B[32m1\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m x, y = \u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_norm\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mint\u001B[39m(y_norm * h)\n\u001B[32m     33\u001B[39m points.append((x, y))\n\u001B[32m     34\u001B[39m cv2.circle(image, (x, y), radius, color, -\u001B[32m1\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: cannot convert float NaN to integer"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c9636ff326937c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
